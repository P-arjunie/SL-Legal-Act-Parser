{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyO2lTy5uazgii+oxhPfKRlb",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/P-arjunie/SL-Legal-Act-Parser/blob/main/Act_extraction.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AMwRYCZz-tvg",
        "outputId": "6f9f1551-83e7-4c84-8dd0-9a5fe82ad3c0"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# =========================================\n",
        "# INSTALL DEPENDENCIES\n",
        "# =========================================\n",
        "!pip install pdfplumber pytesseract scikit-learn pillow beautifulsoup4 lxml > /dev/null\n",
        "\n",
        "import pdfplumber\n",
        "import pytesseract\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "from sklearn.cluster import KMeans\n",
        "import io\n",
        "import json\n",
        "import re"
      ],
      "metadata": {
        "id": "QIwJBPFV-4UQ"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def _get_act_identifier(act):\n",
        "    \"\"\"\n",
        "    Generates a unique identifier for an act based on its official_name and year.\n",
        "    Args:\n",
        "        act (dict): A dictionary representing a parsed act, expected to have 'act_info' with 'official_name' and 'year'.\n",
        "    Returns:\n",
        "        str: A unique identifier string or None if essential information is missing.\n",
        "    \"\"\"\n",
        "    official_name = act.get('act_info', {}).get('official_name')\n",
        "    year = act.get('act_info', {}).get('year')\n",
        "\n",
        "    if official_name and year:\n",
        "        # Normalize name for consistent comparison (e.g., lowercase, remove extra spaces)\n",
        "        normalized_name = re.sub(r'\\s+', ' ', official_name).strip().lower()\n",
        "        return f\"{normalized_name}_{year}\"\n",
        "    return None\n",
        "\n",
        "print(\"'_get_act_identifier' function defined.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kzuD5R-d_Kwj",
        "outputId": "d5322afe-09f7-43b4-b06a-d16a9ae96413"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "'_get_act_identifier' function defined.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "55429274",
        "outputId": "3ce318bd-bba1-4306-e5a5-1eba2750a925"
      },
      "source": [
        "##correct code\n",
        "\n",
        "\n",
        "# =========================================\n",
        "# PDF LAYOUT EXTRACTION FUNCTIONS\n",
        "# =========================================\n",
        "\n",
        "def pdf_page_is_scanned(page):\n",
        "    return not bool(page.extract_text())\n",
        "\n",
        "\n",
        "def ocr_image_from_page(page):\n",
        "    \"\"\"OCR a PDF page and return word bounding boxes.\"\"\"\n",
        "    pil = page.to_image(resolution=180).original  # better OCR accuracy\n",
        "    data = pytesseract.image_to_data(pil, output_type=pytesseract.Output.DICT)\n",
        "\n",
        "    words = []\n",
        "    for i in range(len(data['text'])):\n",
        "        txt = data['text'][i].strip()\n",
        "        if not txt:\n",
        "            continue\n",
        "        x, y, w, h = data['left'][i], data['top'][i], data['width'][i], data['height'][i]\n",
        "        words.append({\n",
        "            'text': txt,\n",
        "            'x0': x, 'x1': x + w,\n",
        "            'y0': y, 'y1': y + h,\n",
        "            'cx': x + w / 2,\n",
        "            'cy': y + h / 2\n",
        "        })\n",
        "    return words\n",
        "\n",
        "\n",
        "def words_from_pdfplumber_page(page):\n",
        "    \"\"\"Extract selectable text with bounding boxes.\"\"\"\n",
        "    words = []\n",
        "    for w in page.extract_words():\n",
        "        words.append({\n",
        "            'text': w[\"text\"],\n",
        "            'x0': float(w[\"x0\"]),\n",
        "            'x1': float(w[\"x1\"]),\n",
        "            'y0': float(w[\"top\"]),\n",
        "            'y1': float(w[\"bottom\"]),\n",
        "            'cx': (float(w[\"x0\"]) + float(w[\"x1\"])) / 2,\n",
        "            'cy': (float(w[\"top\"]) + float(w[\"bottom\"])) / 2,\n",
        "        })\n",
        "    return words\n",
        "\n",
        "\n",
        "def detect_column_clusters(words, n_clusters=2):\n",
        "    xs = np.array([[w['cx']] for w in words])\n",
        "    if xs.max() - xs.min() < 120:  # single-column detection\n",
        "        return np.zeros(len(words), dtype=int)\n",
        "    kmeans = KMeans(n_clusters=n_clusters, random_state=0).fit(xs)\n",
        "    return kmeans.labels_\n",
        "\n",
        "\n",
        "def build_lines_by_column(words, labels):\n",
        "    out_columns = []\n",
        "\n",
        "    for cluster_id in sorted(set(labels)):\n",
        "        cluster_words = [\n",
        "            w for w, lab in zip(words, labels) if lab == cluster_id\n",
        "        ]\n",
        "\n",
        "        cluster_words = sorted(cluster_words, key=lambda w: (w['cy'], w['cx']))\n",
        "\n",
        "        lines = []\n",
        "        current_line = [cluster_words[0]]\n",
        "\n",
        "        for w in cluster_words[1:]:\n",
        "            prev = current_line[-1]\n",
        "            if abs(w['cy'] - prev['cy']) < max(prev['y1'] - prev['y0'], w['y1'] - w['y0']):\n",
        "                current_line.append(w)\n",
        "            else:\n",
        "                lines.append(current_line)\n",
        "                current_line = [w]\n",
        "        lines.append(current_line)\n",
        "\n",
        "        text_lines = [' '.join([t['text'] for t in line]) for line in lines]\n",
        "\n",
        "        blocks = [{\n",
        "            'y': np.mean([t['cy'] for t in line]),\n",
        "            'text': tline\n",
        "        } for line, tline in zip(lines, text_lines)]\n",
        "\n",
        "        out_columns.append(blocks)\n",
        "\n",
        "    return out_columns\n",
        "\n",
        "\n",
        "def merge_columns_reading_order(column_blocks):\n",
        "    merged = []\n",
        "    for col_idx, col in enumerate(column_blocks):\n",
        "        for b in col:\n",
        "            merged.append((b['y'], col_idx, b['text']))\n",
        "\n",
        "    merged_sorted = sorted(merged, key=lambda t: (t[0], t[1]))\n",
        "    return \"\\n\".join([m[2] for m in merged_sorted])\n",
        "\n",
        "\n",
        "def extract_layout_text_from_pdf(path):\n",
        "    all_pages = []\n",
        "\n",
        "    with pdfplumber.open(path) as pdf:\n",
        "        for page in pdf.pages:\n",
        "            if pdf_page_is_scanned(page):\n",
        "                words = ocr_image_from_page(page)\n",
        "            else:\n",
        "                words = words_from_pdfplumber_page(page)\n",
        "\n",
        "            if not words:\n",
        "                all_pages.append(\"\")\n",
        "                continue\n",
        "\n",
        "            labels = detect_column_clusters(words)\n",
        "            col_blocks = build_lines_by_column(words, labels)\n",
        "            ordered_text = merge_columns_reading_order(col_blocks)\n",
        "            all_pages.append(ordered_text)\n",
        "\n",
        "    return \"\\n\\n\".join(all_pages)\n",
        "\n",
        "# =========================================\n",
        "# FIXED PARSER (Sri Lankan Act Format) - Modified to extract long_title and year\n",
        "# =========================================\n",
        "\n",
        "def parse_act_text(text):\n",
        "    lines = [l.strip() for l in text.split(\"\\n\") if l.strip()]\n",
        "\n",
        "    act = {\n",
        "        \"act_info\": {\n",
        "            \"official_name\": None,\n",
        "            \"long_title\": None,\n",
        "            \"year\": None\n",
        "        },\n",
        "        \"chapters\": []\n",
        "    }\n",
        "\n",
        "    if not lines:\n",
        "        return act\n",
        "\n",
        "    # Pre-processing step to merge split section numbers\n",
        "    processed_lines = []\n",
        "    i = 0\n",
        "    while i < len(lines):\n",
        "        current_line = lines[i]\n",
        "        # Check for split section number pattern: \"1\" followed by \". This Act...\"\n",
        "        if re.fullmatch(r'\\d+', current_line.strip()) and i + 1 < len(lines) and lines[i+1].strip().startswith('.'):\n",
        "            # Merge the current line (number) with the next line (starts with .)\n",
        "            merged_line = current_line.strip() + lines[i+1].strip()\n",
        "            processed_lines.append(merged_line)\n",
        "            i += 2 # Skip the next line as it's been merged\n",
        "        else:\n",
        "            processed_lines.append(current_line)\n",
        "            i += 1\n",
        "    lines = processed_lines\n",
        "\n",
        "    # Extract official name (first non-empty line)\n",
        "    act[\"act_info\"][\"official_name\"] = lines[0]\n",
        "    line_idx = 1 # Start checking from the second line\n",
        "\n",
        "    # Collect long title\n",
        "    long_title_parts = []\n",
        "    # Keywords indicating the end of the long title and start of operational text\n",
        "    # 'AN ACT TO' should now be part of the long title, so it's removed from end patterns.\n",
        "    long_title_end_patterns = [\n",
        "        re.compile(r\"^(BE IT ENACTED|THE PARLIAMENT OF|PROVIDING FOR)\", re.IGNORECASE), # 'AN ACT TO' removed\n",
        "        re.compile(r\"^Short title and dates of operation\\.$\", re.IGNORECASE),\n",
        "        re.compile(r\"^\\d+\\.\\s*\"), # First section\n",
        "        re.compile(r\"CHAPTER\\s+([A-Z]+)\", re.I)\n",
        "    ]\n",
        "\n",
        "    while line_idx < len(lines):\n",
        "        line = lines[line_idx]\n",
        "        should_break = False\n",
        "        for pattern in long_title_end_patterns:\n",
        "            if pattern.match(line):\n",
        "                should_break = True\n",
        "                break\n",
        "        if should_break:\n",
        "            break\n",
        "        long_title_parts.append(line)\n",
        "        line_idx += 1\n",
        "\n",
        "    if long_title_parts:\n",
        "        act[\"act_info\"][\"long_title\"] = \"\\n\".join(long_title_parts)\n",
        "\n",
        "    current_chapter = {\n",
        "        \"chapter_number\": None,\n",
        "        \"chapter_title\": None,\n",
        "        \"sections\": []\n",
        "    }\n",
        "\n",
        "    prev_line = None\n",
        "    current_section = None\n",
        "\n",
        "    # Continue parsing from where long title collection stopped\n",
        "    for i in range(line_idx, len(lines)):\n",
        "        line = lines[i]\n",
        "\n",
        "        # Year extraction is deferred to post-processing\n",
        "\n",
        "        # ---------- CHAPTER ----------\n",
        "        chap = re.match(r\"CHAPTER\\s+([A-Z]+)\", line, re.I)\n",
        "        if chap:\n",
        "            if current_section: # if there's an ongoing section, add it to current chapter\n",
        "                current_chapter[\"sections\"].append(current_section)\n",
        "                current_section = None # reset current_section\n",
        "\n",
        "            if current_chapter[\"sections\"]: # if current chapter has sections, add it to act\n",
        "                act[\"chapters\"].append(current_chapter)\n",
        "\n",
        "            current_chapter = {\n",
        "                \"chapter_number\": chap.group(1),\n",
        "                \"chapter_title\": None,\n",
        "                \"sections\": []\n",
        "            }\n",
        "            prev_line = None\n",
        "            continue\n",
        "\n",
        "        # ---------- SECTION ----------\n",
        "        # Regex for section: starts with digit(s) followed by a period and space\n",
        "        sec = re.match(r\"^(\\d+)\\.\\s*(.*)\", line)\n",
        "        if sec:\n",
        "            if current_section:\n",
        "                current_chapter[\"sections\"].append(current_section)\n",
        "\n",
        "            sec_num = sec.group(1)\n",
        "            body_after = sec.group(2).strip()\n",
        "\n",
        "            # Determine section heading: It's usually the line immediately preceding the section number,\n",
        "            # but only if it's not a subsection or continuation of previous text.\n",
        "            heading = None\n",
        "            if prev_line and not prev_line.endswith(\":\"):\n",
        "                # Ensure it's not a subsection or other structured element\n",
        "                if not re.match(r\"^(?:\\(|[a-z]\\))\", prev_line) and not re.match(r\"^(?:\\d+|[a-z]|[ivx]+)\\)\\s*\", prev_line):\n",
        "                     heading = prev_line\n",
        "\n",
        "            current_section = {\n",
        "                \"section_number\": sec_num,\n",
        "                \"section_heading\": heading,\n",
        "                \"section_body\": body_after,\n",
        "                \"subsections\": []\n",
        "            }\n",
        "\n",
        "            prev_line = None # Reset prev_line after a new section starts\n",
        "            continue\n",
        "\n",
        "        # ---------- SUBSECTIONS ----------\n",
        "        sub = re.match(r\"^\\((\\d+|[a-z]|[ivx]+)\\)\\s*(.*)\", line, re.I)\n",
        "        if sub and current_section:\n",
        "            current_section[\"subsections\"].append({\n",
        "                \"sub_number\": f\"({sub.group(1)})\",\n",
        "                \"text\": sub.group(2)\n",
        "            })\n",
        "            prev_line = None # Reset prev_line for consistency\n",
        "            continue\n",
        "\n",
        "        # ---------- BODY TEXT ----------\n",
        "        if current_section:\n",
        "            # Append to section body if not a duplicate of prev_line (e.g., from double newlines)\n",
        "            if line != prev_line: # prevent appending same line twice\n",
        "                current_section[\"section_body\"] += \"\\n\" + line\n",
        "            prev_line = line # Update prev_line\n",
        "        else:\n",
        "            # If we are outside a section/chapter and not collecting long_title,\n",
        "            # this line might be an unparsed preamble or just noise.\n",
        "            prev_line = line\n",
        "\n",
        "\n",
        "    # finalize last section/chapter\n",
        "    if current_section:\n",
        "        current_chapter[\"sections\"].append(current_section)\n",
        "\n",
        "    if current_chapter[\"sections\"]:\n",
        "        act[\"chapters\"].append(current_chapter)\n",
        "\n",
        "    # Post-processing to extract year from collected data\n",
        "    # More robust year extraction after all text is structured.\n",
        "    year_found = False\n",
        "\n",
        "    # 1. Check official name\n",
        "    year_match = re.search(r'(\\d{4})', act[\"act_info\"][\"official_name\"])\n",
        "    if year_match:\n",
        "        act[\"act_info\"][\"year\"] = int(year_match.group(1))\n",
        "        year_found = True\n",
        "\n",
        "    # 2. Check long title\n",
        "    if not year_found and act[\"act_info\"][\"long_title\"]:\n",
        "        year_match = re.search(r'(\\d{4})', act[\"act_info\"][\"long_title\"])\n",
        "        if year_match:\n",
        "            act[\"act_info\"][\"year\"] = int(year_match.group(1))\n",
        "            year_found = True\n",
        "\n",
        "    # 3. Check first section's body (most common place for citation year)\n",
        "    if not year_found and act[\"chapters\"] and act[\"chapters\"][0][\"sections\"]:\n",
        "        first_section_body = act[\"chapters\"][0][\"sections\"][0][\"section_body\"]\n",
        "        # Look for patterns like \"No. XX of YYYY\" or just \"YYYY\"\n",
        "        year_match = re.search(r'\\b(?:No\\.\\s+\\d+\\s+of\\s+)?(\\d{4})\\b', first_section_body)\n",
        "        if year_match:\n",
        "            act[\"act_info\"][\"year\"] = int(year_match.group(1))\n",
        "            year_found = True\n",
        "\n",
        "    return act\n",
        "\n",
        "# Helper function to generate a unique identifier for an act\n",
        "def _get_act_identifier(act):\n",
        "    \"\"\"\n",
        "    Generates a unique identifier for an act based on its official_name and year.\n",
        "    Args:\n",
        "        act (dict): A dictionary representing a parsed act, expected to have 'act_info' with 'official_name' and 'year'.\n",
        "    Returns:\n",
        "        str: A unique identifier string or None if essential information is missing.\n",
        "    \"\"\"\n",
        "    official_name = act.get('act_info', {}).get('official_name')\n",
        "    year = act.get('act_info', {}).get('year')\n",
        "\n",
        "    if official_name and year:\n",
        "        # Normalize name for consistent comparison (e.g., lowercase, remove extra spaces)\n",
        "        normalized_name = re.sub(r'\\s+', ' ', official_name).strip().lower()\n",
        "        return f\"{normalized_name}_{year}\"\n",
        "    return None\n",
        "\n",
        "# =========================================\n",
        "# HTML EXTRACTION\n",
        "# =========================================\n",
        "\n",
        "def extract_html_text(url):\n",
        "    response = requests.get(url)\n",
        "    soup = BeautifulSoup(response.content, 'html.parser')\n",
        "\n",
        "    for bad in soup(['script', 'style', 'header', 'footer', 'nav']):\n",
        "        bad.decompose()\n",
        "\n",
        "    content = (\n",
        "        soup.find(\"div\", class_=\"entry-content\")\n",
        "        or soup.find(\"div\", class_=\"post-content\")\n",
        "        or soup.find(\"article\")\n",
        "        or soup.find(\"div\", id=\"content\")\n",
        "        or soup.body\n",
        "    )\n",
        "\n",
        "    return content.get_text(separator=\"\\n\", strip=True)\n",
        "\n",
        "\n",
        "# =========================================\n",
        "# MAIN EXECUTION FLOW\n",
        "# =========================================\n",
        "\n",
        "url = input(\"Enter the URL for the legal act (PDF or HTML): \").strip()\n",
        "all_parsed_acts = []\n",
        "\n",
        "if url:\n",
        "    print(\"Fetching document…\")\n",
        "\n",
        "    if url.lower().endswith(\".pdf\"):\n",
        "        print(\"Downloading PDF…\")\n",
        "        pdf_bytes = requests.get(url).content\n",
        "        temp_path = \"/content/temp_law.pdf\"\n",
        "        with open(temp_path, \"wb\") as f:\n",
        "            f.write(pdf_bytes)\n",
        "\n",
        "        print(\"Extracting layout-aware text (OCR if needed)…\")\n",
        "        raw_text = extract_layout_text_from_pdf(temp_path)\n",
        "\n",
        "    else:\n",
        "        print(\"Extracting text from HTML…\")\n",
        "        raw_text = extract_html_text(url)\n",
        "\n",
        "    print(\"\\n=== Extracted Text (first 1500 chars) ===\")\n",
        "    print(raw_text[:1500])\n",
        "    print(\"\\n=========================================\\n\")\n",
        "\n",
        "    print(\"Parsing Act Structure…\")\n",
        "    parsed = parse_act_text(raw_text)\n",
        "\n",
        "    all_parsed_acts.append(parsed)\n",
        "\n",
        "    print(\"=== Parsed JSON Preview (first part) ===\")\n",
        "    print(json.dumps(parsed, indent=2, ensure_ascii=False)[:2000])\n",
        "\n",
        "else:\n",
        "    print(\"No URL entered.\")\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "import os\n",
        "import json\n",
        "\n",
        "# Define the output file name, now pointing to Google Drive\n",
        "output_filename = \"/content/drive/MyDrive/parsed_act.json\"\n",
        "\n",
        "# Initialize an empty list for the final acts\n",
        "final_acts_to_save = []\n",
        "\n",
        "# Check if the output file exists and load its content\n",
        "if os.path.exists(output_filename):\n",
        "    with open(output_filename, 'r') as f:\n",
        "        final_acts_to_save = json.load(f)\n",
        "    print(f\"Loaded {len(final_acts_to_save)} existing acts from {output_filename}\")\n",
        "else:\n",
        "    print(f\"No existing acts found at {output_filename}. Initializing an empty list.\")\n",
        "\n",
        "print(f\"Current state of final_acts_to_save: {len(final_acts_to_save)} items.\")\n",
        "\n",
        "unique_act_identifiers = set()\n",
        "# Populate the set with identifiers from already loaded acts\n",
        "for act in final_acts_to_save:\n",
        "    identifier = _get_act_identifier(act)\n",
        "    if identifier:\n",
        "        unique_act_identifiers.add(identifier)\n",
        "\n",
        "print(f\"Initial unique identifiers from loaded acts: {len(unique_act_identifiers)}\")\n",
        "\n",
        "# Integrate newly parsed acts (from all_parsed_acts) with duplicate checking\n",
        "newly_added_count = 0\n",
        "for act in all_parsed_acts:\n",
        "    identifier = _get_act_identifier(act)\n",
        "    if identifier:\n",
        "        if identifier not in unique_act_identifiers:\n",
        "            final_acts_to_save.append(act)\n",
        "            unique_act_identifiers.add(identifier)\n",
        "            newly_added_count += 1\n",
        "        else:\n",
        "            print(f\"Skipping duplicate act: {identifier}\")\n",
        "    else:\n",
        "        print(f\"Skipping act due to missing identifier info: {act.get('act_info', {})}\")\n",
        "\n",
        "print(f\"Added {newly_added_count} new unique acts. Total acts in collection: {len(final_acts_to_save)}\")\n",
        "\n",
        "\n",
        "\n",
        "with open(output_filename, 'w', encoding='utf-8') as f:\n",
        "    json.dump(final_acts_to_save, f, indent=2, ensure_ascii=False)\n",
        "\n",
        "print(f\"Successfully saved {len(final_acts_to_save)} unique acts to {output_filename}\")"
      ],
      "execution_count": 132,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter the URL for the legal act (PDF or HTML): http://www.lawnet.gov.lk/wp-content/uploads/Law%20Site/4-stats_1956_2006/set3/1985Y0V0C29A.html\n",
            "Fetching document…\n",
            "Extracting text from HTML…\n",
            "\n",
            "=== Extracted Text (first 1500 chars) ===\n",
            "Declaration Of Assets And Liabilities (Amendment)\n",
            "AN ACT TO AMEND THE DECLARATION OF ASSETS AND LIABILITIES LAW, No.1 OF 1975.\n",
            "BE it enacted by the Parliament of the Democratic Socialist Republic of Sri Lanka as follows:-\n",
            "[\n",
            "6\n",
            "th\n",
            "August\n",
            "\t\t, 1985\n",
            "]\n",
            "Short title.\n",
            "1\n",
            ". This Act may be cited as the Declaration of Assets and Liabilities (Amendment) Act, No. 29 of 1985.\n",
            "Amendment of section 12 of Law No. 1 of 1975.\n",
            "2\n",
            ". Section 12 of the Declaration of Assets and Liabilities 1975, Law, No. 1 of 1975, is hereby amended by the substitution for the definition of \"staff officer to, of the following new definition:\n",
            "\" staff officer\" means\n",
            "(a) in the case of any employee of the Government or a local authority, any officer holding any office the initial of the salary scale of which is Rs. 13,800 per annum or above and whose annual increments are Rs. 480 and above; and\n",
            "(b) in the case of any employee of a public corporation, any officer holding any office the initial of the salary scale of which is not less than Rs. 9,600 per annum or any other salary determined by resolution of Parliament. '.\n",
            "\n",
            "=========================================\n",
            "\n",
            "Parsing Act Structure…\n",
            "=== Parsed JSON Preview (first part) ===\n",
            "{\n",
            "  \"act_info\": {\n",
            "    \"official_name\": \"Declaration Of Assets And Liabilities (Amendment)\",\n",
            "    \"long_title\": \"AN ACT TO AMEND THE DECLARATION OF ASSETS AND LIABILITIES LAW, No.1 OF 1975.\",\n",
            "    \"year\": 1975\n",
            "  },\n",
            "  \"chapters\": [\n",
            "    {\n",
            "      \"chapter_number\": null,\n",
            "      \"chapter_title\": null,\n",
            "      \"sections\": [\n",
            "        {\n",
            "          \"section_number\": \"1\",\n",
            "          \"section_heading\": \"Short title.\",\n",
            "          \"section_body\": \"This Act may be cited as the Declaration of Assets and Liabilities (Amendment) Act, No. 29 of 1985.\\nAmendment of section 12 of Law No. 1 of 1975.\",\n",
            "          \"subsections\": []\n",
            "        },\n",
            "        {\n",
            "          \"section_number\": \"2\",\n",
            "          \"section_heading\": \"Amendment of section 12 of Law No. 1 of 1975.\",\n",
            "          \"section_body\": \"Section 12 of the Declaration of Assets and Liabilities 1975, Law, No. 1 of 1975, is hereby amended by the substitution for the definition of \\\"staff officer to, of the following new definition:\\n\\\" staff officer\\\" means\",\n",
            "          \"subsections\": [\n",
            "            {\n",
            "              \"sub_number\": \"(a)\",\n",
            "              \"text\": \"in the case of any employee of the Government or a local authority, any officer holding any office the initial of the salary scale of which is Rs. 13,800 per annum or above and whose annual increments are Rs. 480 and above; and\"\n",
            "            },\n",
            "            {\n",
            "              \"sub_number\": \"(b)\",\n",
            "              \"text\": \"in the case of any employee of a public corporation, any officer holding any office the initial of the salary scale of which is not less than Rs. 9,600 per annum or any other salary determined by resolution of Parliament. '.\"\n",
            "            }\n",
            "          ]\n",
            "        }\n",
            "      ]\n",
            "    }\n",
            "  ]\n",
            "}\n",
            "Loaded 252 existing acts from /content/drive/MyDrive/parsed_act.json\n",
            "Current state of final_acts_to_save: 252 items.\n",
            "Initial unique identifiers from loaded acts: 252\n",
            "Skipping duplicate act: declaration of assets and liabilities (amendment)_1975\n",
            "Added 0 new unique acts. Total acts in collection: 252\n",
            "Successfully saved 252 unique acts to /content/drive/MyDrive/parsed_act.json\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import json\n",
        "\n",
        "# Define the output file name, now pointing to Google Drive\n",
        "output_filename = \"/content/drive/MyDrive/parsed_act.json\"\n",
        "\n",
        "# Initialize an empty list for the final acts\n",
        "final_acts_to_save = []\n",
        "\n",
        "# Check if the output file exists and load its content\n",
        "if os.path.exists(output_filename):\n",
        "    with open(output_filename, 'r') as f:\n",
        "        final_acts_to_save = json.load(f)\n",
        "    print(f\"Loaded {len(final_acts_to_save)} existing acts from {output_filename}\")\n",
        "else:\n",
        "    print(f\"No existing acts found at {output_filename}. Initializing an empty list.\")\n",
        "\n",
        "print(f\"Current state of final_acts_to_save: {len(final_acts_to_save)} items.\")\n",
        "\n",
        "unique_act_identifiers = set()\n",
        "# Populate the set with identifiers from already loaded acts\n",
        "for act in final_acts_to_save:\n",
        "    identifier = _get_act_identifier(act)\n",
        "    if identifier:\n",
        "        unique_act_identifiers.add(identifier)\n",
        "\n",
        "print(f\"Initial unique identifiers from loaded acts: {len(unique_act_identifiers)}\")\n",
        "\n",
        "# Integrate newly parsed acts (from all_parsed_acts) with duplicate checking\n",
        "newly_added_count = 0\n",
        "for act in all_parsed_acts:\n",
        "    identifier = _get_act_identifier(act)\n",
        "    if identifier:\n",
        "        if identifier not in unique_act_identifiers:\n",
        "            final_acts_to_save.append(act)\n",
        "            unique_act_identifiers.add(identifier)\n",
        "            newly_added_count += 1\n",
        "        else:\n",
        "            print(f\"Skipping duplicate act: {identifier}\")\n",
        "    else:\n",
        "        print(f\"Skipping act due to missing identifier info: {act.get('act_info', {})}\")\n",
        "\n",
        "print(f\"Added {newly_added_count} new unique acts. Total acts in collection: {len(final_acts_to_save)}\")\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "with open(output_filename, 'w', encoding='utf-8') as f:\n",
        "    json.dump(final_acts_to_save, f, indent=2, ensure_ascii=False)\n",
        "\n",
        "print(f\"Successfully saved {len(final_acts_to_save)} unique acts to {output_filename}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d7BMt-x5-eeO",
        "outputId": "db00e0a5-7a84-41d3-9631-1577e0fa06c3"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded 148 existing acts from /content/drive/MyDrive/parsed_act.json\n",
            "Current state of final_acts_to_save: 148 items.\n",
            "Initial unique identifiers from loaded acts: 148\n",
            "Skipping duplicate act: ceylonese evangelistic association (incorporation) law_1975\n",
            "Added 0 new unique acts. Total acts in collection: 148\n",
            "Successfully saved 148 unique acts to /content/drive/MyDrive/parsed_act.json\n"
          ]
        }
      ]
    }
  ]
}